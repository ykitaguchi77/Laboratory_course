{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL4UH6b20t9B9ZOr7CUlQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Laboratory_course/blob/master/11.%20GradCAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM 診断根拠の可視化**\n",
        "\n",
        "https://qiita.com/m__k/items/0a841a1a93d70a663c39\n",
        "\n",
        "https://zenn.dev/iq108uni/articles/7269a1b72f42be"
      ],
      "metadata": {
        "id": "5C238zngj9i7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7LD1_Rygj7cO"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "gXU5WIP26kgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#dataset.zipを解凍\n",
        "\n",
        "!date -R\n",
        "!unzip -qq drive/My\\ Drive/AI_laboratory_course/dataset.zip\n",
        "!date -R\n",
        "!ls\n",
        "\n",
        "shutil.move(\"/content/train/appl\", \"/content/appl\")\n",
        "shutil.move(\"/content/train/stra\", \"/content/stra\")\n",
        "shutil.rmtree(\"/content/train\")\n",
        "shutil.rmtree(\"/content/val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPWHfJPgkClS",
        "outputId": "da3acfd6-b144-4cbe-c67c-333bbe122dcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Tue, 23 Jan 2024 01:24:38 +0000\n",
            "Tue, 23 Jan 2024 01:24:43 +0000\n",
            "drive  sample_data  train  val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trainとvalにランダムに分けるスクリプト\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class_names = [\"appl\", \"stra\"] ###←ここを変更してください\n",
        "parent_dir = f'/content' ###←ここを変更してください\n",
        "model_path = '/content/drive/My Drive/AI_laboratory_course/classification.pth' ###←ここを変更してください\n",
        "\n",
        "\n",
        "\n",
        "for class_name in class_names:\n",
        "\n",
        "    # location of the dataset\n",
        "    orig_dir = f'{parent_dir}/{class_name}'\n",
        "\n",
        "    # get the list of all files in the folder\n",
        "    all_files = glob.glob(f\"{orig_dir}/*\")\n",
        "\n",
        "    print(f\"len_all_files: {len(all_files)}\")\n",
        "\n",
        "    # split the list into train and validation sets\n",
        "    train_files, val_files = train_test_split(all_files, test_size=0.2, random_state=0)\n",
        "\n",
        "    # create the train and validation folders if they don't exist\n",
        "    train_folder = f'/content/train/{class_name}'\n",
        "    val_folder = f'/content/val/{class_name}'\n",
        "    if os.path.exists(train_folder):\n",
        "        shutil.rmtree(train_folder)\n",
        "    os.makedirs(train_folder)\n",
        "    if os.path.exists(val_folder):\n",
        "        shutil.rmtree(val_folder)\n",
        "    os.makedirs(val_folder)\n",
        "\n",
        "    # copy the files from the dataset folder to the train and validation folders\n",
        "    for file in train_files:\n",
        "        new_location = f\"{train_folder}/{os.path.basename(file)}\"\n",
        "        shutil.copy(file, new_location)\n",
        "\n",
        "    for file in val_files:\n",
        "        new_location = os.path.join(val_folder, os.path.basename(file))\n",
        "        shutil.copy(file, new_location)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhqn7E27lNlv",
        "outputId": "75239375-2c4a-47a9-8704-16f1e10afeee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len_all_files: 252\n",
            "len_all_files: 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "data_dir = '/content'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path= data_dir + '/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path= data_dir + '/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi2M2-Q8kPLz",
        "outputId": "1a2bdc38-b6b3-461a-97f1-3355bd453d03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['appl', 'stra']\n",
            "appl_train:201\n",
            "stra_train:201\n",
            "appl_val:51\n",
            "stra_val:51\n",
            "training data set_total：402\n",
            "validating data set_total：102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**GradCAM**"
      ],
      "metadata": {
        "id": "T_qrzax4kWzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "data_dir = '/content'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path= data_dir + '/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path= data_dir + '/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScNEp0yPkadj",
        "outputId": "1a6ce993-c368-4ce8-ab21-4bb9cc995d31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['appl', 'stra']\n",
            "appl_train:201\n",
            "stra_train:201\n",
            "appl_val:51\n",
            "stra_val:51\n",
            "training data set_total：402\n",
            "validating data set_total：102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device) #GPU使用\n",
        "\n",
        "model_ft.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgh8FEy7krai",
        "outputId": "14ebd52b-9f07-4ac3-b31b-818d79a0e218"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 154MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#########################\n",
        "## ここからGradCAM\n",
        "#########################\n",
        "\n",
        "\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "def GradCAM(img, c, features_fn, classifier_fn): #cはclass\n",
        "    feats = features_fn(img.cuda()) #imgをforwardする\n",
        "    _, N, H, W = feats.size() #[1,512,7,7] 7*7のfeatureが512層ある\n",
        "\n",
        "    out = classifier_fn(feats)\n",
        "    #print(f\"out: {out}\") #out: tensor([[ 1.1058, -1.4082]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
        "    c_score = out[0, c]   #該当するclassのscore (model_ftを通した場合)\n",
        "\n",
        "    grads = torch.autograd.grad(c_score, feats)   #featsの勾配を計算\n",
        "\n",
        "    print(f\"grads[0].shape: {grads[0].shape}\") #torch.Size([1, 512, 7, 7])\n",
        "    print(f\"grads[0][0].shape: {grads[0][0].shape}\") #torch.Size([512, 7, 7])\n",
        "\n",
        "    w = grads[0][0].mean(-1).mean(-1)           #ここでGlobalAveragePoolingをしている（各featureについて7*7全ての平均をとる）\n",
        "\n",
        "    #print(f\"w.shape: {grads[0][0].mean(-1).mean(-1).shape}\") #torch.Size([512])\n",
        "    #print(feats.view(N, H*W).shape) #torch.Size([512, 49])\n",
        "\n",
        "    sal = torch.matmul(w, feats.view(N, H*W))  #torch.Size([49])\n",
        "    sal = sal.view(H, W).cpu().detach().numpy() #(7,7)\n",
        "    sal = np.maximum(sal, 0) #ReLUと同じ\n",
        "    return sal\n",
        "\n",
        "read_tensor = transforms.Compose([\n",
        "    lambda x: Image.open(x),\n",
        "    lambda x: x.convert('RGB'),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    lambda x: torch.unsqueeze(x, 0) #次元を1に引き延ばす\n",
        "])\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      #print('Image: '+ image_name)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      #print('Label: '+ label)\n",
        "      return(image_name, label)"
      ],
      "metadata": {
        "id": "sQTUmyBJmQCW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split model in two parts\n",
        "features_fn = nn.Sequential(*list(model_ft.children())[:-2]) #最後の2層（AdaptiveAvgPool2dとLinear)を取り除いたもの\n",
        "classifier_fn = nn.Sequential(*(list(model_ft.children())[-2:-1] + [Flatten()] + list(model_ft.children())[-1:])) #最終層の前にFlatten()を挿入\n",
        "#最後の2層"
      ],
      "metadata": {
        "id": "zIoFM0APJMw2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft"
      ],
      "metadata": {
        "id": "fGFgL3XXpFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_fn"
      ],
      "metadata": {
        "id": "HdCyUo16JOin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_fn"
      ],
      "metadata": {
        "id": "6HPRNJf1JWRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split model in two parts\n",
        "features_fn = nn.Sequential(*list(model_ft.children())[:-2]) #最後の2層（AdaptiveAvgPool2dとLinear)を取り除いたもの\n",
        "classifier_fn = nn.Sequential(*(list(model_ft.children())[-2:-1] + [Flatten()] + list(model_ft.children())[-1:])) #最終層の前にFlatten()を挿入\n",
        " #最後の2層\n",
        "\n",
        "#評価モードにする\n",
        "model_ft = model_ft.eval()\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "\n",
        "#画像のパスを指定\n",
        "for j in range(10):\n",
        "#for j in range(len(image_datasets[\"val\"])):\n",
        "\n",
        "    #元画像\n",
        "    image = image_datasets[\"val\"][j][0]\n",
        "    image = image.permute(1, 2, 0)\n",
        "\n",
        "    img_tensor = image_datasets[\"val\"][j][0].unsqueeze(0)\n",
        "    #Softmaxにかけたときの確率上位1つのpp(確率)とcc(class番号)を取得(0→apple,1→stra)\n",
        "    pp, cc = torch.topk(nn.Softmax(dim=1)(model_ft(img_tensor.to(device))), 1)\n",
        "    #print(pp, cc) #tensor([[0.9251]], device='cuda:0', grad_fn=<TopkBackward0>) tensor([[0]], device='cuda:0')\n",
        "\n",
        "    #pとcを対にして入力\n",
        "    for i, (p, c) in enumerate(zip(pp[0], cc[0])):  #これでtensorを外す\n",
        "        #print(p, c) #tensor(0.9251, device='cuda:0', grad_fn=<UnbindBackward0>) tensor(0, device='cuda:0')\n",
        "        sal = GradCAM(img_tensor, int(c), features_fn, classifier_fn)\n",
        "        tmp = image.to('cpu').detach().numpy().copy()\n",
        "        img = Image.fromarray((tmp*255).astype(np.uint8))\n",
        "        #TensorをImageに変換\n",
        "        sal = Image.fromarray(sal)\n",
        "        sal = sal.resize(img.size, resample=Image.LINEAR)\n",
        "\n",
        "        print()\n",
        "        #print(img_path) #あとで参照しやすいように画像のパスを表示\n",
        "\n",
        "        #plt.title('')\n",
        "        print('label: '+class_names[image_datasets[\"val\"][j][1]])\n",
        "        print('pred:  '+'{}  {:.1f}%'.format(class_names[c], 100*float(p)))\n",
        "\n",
        "        plt.figure(figsize=(6, 4))\n",
        "\n",
        "        #グラフを1行2列に並べたうちの1番目\n",
        "        plt.subplots_adjust(wspace=0,hspace=0)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.imshow(np.array(sal), alpha=0.5, cmap='jet')\n",
        "\n",
        "        #元の画像を並べて表示\n",
        "        image = image_datasets[\"val\"][j][0]\n",
        "        image = image.permute(1, 2, 0)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y3TivwJnl_OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**何をしているのかを確認**"
      ],
      "metadata": {
        "id": "VFK7ge5rUOnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**微分**\n",
        "\n",
        "torch.autograd.grad(outputs=y, inputs=x, create_graph=True)"
      ],
      "metadata": {
        "id": "BQOYXoYjt7nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**3\n",
        "grads = torch.autograd.grad(outputs=y, inputs=x, create_graph=True)\n",
        "print(grads)\n",
        "print(grads[0])"
      ],
      "metadata": {
        "id": "oNunB20qTvxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x**3\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "id": "i5LCC4TEUfDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Global Average Pooling**\n",
        "\n",
        "w = grads[0][0].mean(-1).mean(-1)"
      ],
      "metadata": {
        "id": "NTxHO0-Auqvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 形状が[6, 4, 3]のテンソルの作成\n",
        "x = torch.tensor([\n",
        "    [\n",
        "        [1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]\n",
        "    ],\n",
        "    [\n",
        "        [13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24]\n",
        "    ],\n",
        "    [\n",
        "        [25, 26, 27], [28, 29, 30], [31, 32, 33], [34, 35, 36]\n",
        "    ],\n",
        "    [\n",
        "        [37, 38, 39], [40, 41, 42], [43, 44, 45], [46, 47, 48]\n",
        "    ],\n",
        "    [\n",
        "        [49, 50, 51], [52, 53, 54], [55, 56, 57], [58, 59, 60]\n",
        "    ],\n",
        "    [\n",
        "        [61, 62, 63], [64, 65, 66], [67, 68, 69], [70, 71, 72]\n",
        "    ]\n",
        "], dtype=torch.float)\n",
        "\n",
        "x.size()"
      ],
      "metadata": {
        "id": "YqQZCUZIqkaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.mean(-1))\n",
        "print(x.mean(-1).size())"
      ],
      "metadata": {
        "id": "65Efqmb0rLzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.mean(-1).mean(-1))\n",
        "print(x.mean(-1).mean(-1).size())"
      ],
      "metadata": {
        "id": "ikMKEaAmrL1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}