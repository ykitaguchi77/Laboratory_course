{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5Vj5WFaq9eflRQLYbdNqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/Laboratory_course/blob/master/4_%E7%94%BB%E5%83%8F%E3%81%AE%E3%82%B9%E3%82%AF%E3%83%AC%E3%82%A4%E3%83%94%E3%83%B3%E3%82%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**画像のスクレイピング**\n",
        "\n"
      ],
      "metadata": {
        "id": "25UMQJsL7UPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**iCrawlerを使う方法**\n",
        "\n",
        "https://atmarkit.itmedia.co.jp/ait/articles/2010/28/news018.html\n",
        "\n",
        "公式： https://icrawler.readthedocs.io/en/latest/builtin.html"
      ],
      "metadata": {
        "id": "iMH1ZVOtriop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: google driveに接続\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Uh8ovqNhXfg2",
        "outputId": "bedd6865-ac41-40b8-e822-7b464a0928d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# List of keywords\n",
        "keywords = [\"カツオ\", \"マグロ\"]\n",
        "max_num = 350\n",
        "\n",
        "for keyword in keywords:\n",
        "    output_dir = f\"/content/drive/MyDrive/AI_laboratory_course/image/{keyword}\"\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    shutil.rmtree(output_dir)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    crawler = BingImageCrawler(storage={\"root_dir\": output_dir})\n",
        "    crawler.crawl(keyword=keyword, max_num=max_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfZC5Uv_r87o",
        "outputId": "6acd77e3-77f9-4b44-f10c-b6dd274da5f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.11/dist-packages (0.6.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from icrawler) (4.12.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (from icrawler) (0.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from icrawler) (5.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from icrawler) (11.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from icrawler) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Requestsを用いる方法**"
      ],
      "metadata": {
        "id": "R975p35iSWJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "def scrape_google_images(query, start_page=0, max_images=100):\n",
        "    user_agents = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
        "        # Other user agents can be added here\n",
        "    ]\n",
        "\n",
        "    headers = {\"User-Agent\": user_agents[0]}\n",
        "    images = []\n",
        "\n",
        "    while len(images) < max_images:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"tbm\": \"isch\",\n",
        "            \"hl\": \"en\",\n",
        "            \"gl\": \"us\",\n",
        "            \"start\": start_page\n",
        "        }\n",
        "\n",
        "        html = requests.get(\"https://google.com/search\", params=params, headers=headers, timeout=30)\n",
        "        soup = BeautifulSoup(html.text, \"lxml\")\n",
        "\n",
        "        # Extracting inline JSON from script tags\n",
        "        all_script_tags = soup.select(\"script\")\n",
        "        matched_images_data = \"\".join(re.findall(r\"AF_initDataCallback\\(([^<]+)\\);\", str(all_script_tags)))\n",
        "\n",
        "        # Parsing JSON data\n",
        "        matched_images_data_fix = json.dumps(matched_images_data)\n",
        "        matched_images_data_json = json.loads(matched_images_data_fix)\n",
        "\n",
        "        # Extracting image data\n",
        "        matched_google_image_data = re.findall(r'\\\"b-GRID_STATE0\\\"(.*)sideChannel:\\s?{}}', matched_images_data_json)\n",
        "\n",
        "        # Extracting full resolution images\n",
        "        removed_matched_google_images_thumbnails = re.sub(\n",
        "                r'\\[\\\"(https\\:\\/\\/encrypted-tbn0\\.gstatic\\.com\\/images\\?.*?)\\\",\\d+,\\d+\\]', \"\", str(matched_google_image_data))\n",
        "        matched_google_full_resolution_images = re.findall(r\"(?:'|,),\\[\\\"(https:|http.*?)\\\",\\d+,\\d+\\]\", removed_matched_google_images_thumbnails)\n",
        "        full_res_images = [bytes(bytes(img, \"ascii\").decode(\"unicode-escape\"), \"ascii\").decode(\"unicode-escape\") for img in matched_google_full_resolution_images]\n",
        "\n",
        "        images.extend(full_res_images)\n",
        "\n",
        "        if len(images) >= max_images:\n",
        "            break\n",
        "\n",
        "        start_page += 10  # Update the start page to get the next set of results\n",
        "\n",
        "    return images[:max_images]\n",
        "\n",
        "def download_images(keywords, max_num):\n",
        "    \"\"\"\n",
        "    Download images for each keyword and save them in separate folders.\n",
        "    \"\"\"\n",
        "    failed_downloads = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        images = scrape_google_images(keyword, max_images=max_num)\n",
        "        folder_name = keyword.replace(\" \", \"_\")\n",
        "        if not os.path.exists(folder_name):\n",
        "            os.makedirs(folder_name)\n",
        "\n",
        "        for i, img_url in enumerate(tqdm(images, desc=f\"Downloading images for {keyword}\")):\n",
        "            print(img_url)\n",
        "            try:\n",
        "                response = requests.get(img_url)\n",
        "                if response.status_code == 200:\n",
        "                    image = Image.open(BytesIO(response.content))\n",
        "                    image.save(os.path.join(folder_name, f\"{keyword}_{i}.png\"))\n",
        "                else:\n",
        "                    raise Exception(f\"HTTP status code {response.status_code}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to download image {i} for {keyword}: {e}\")\n",
        "                failed_downloads.append((keyword, i))\n",
        "\n",
        "    if failed_downloads:\n",
        "        print(\"Failed to download the following images:\")\n",
        "        for keyword, img_id in failed_downloads:\n",
        "            print(f\"Keyword: {keyword}, Image ID: {img_id}\")\n",
        "    else:\n",
        "        print(\"All images downloaded successfully.\")\n",
        "\n",
        "# Using the function\n",
        "keywords = [\"りんご\", \"みかん\"]\n",
        "max_num = 10\n",
        "download_images(keywords, max_num)\n"
      ],
      "metadata": {
        "id": "qFdAbzvEhY--",
        "outputId": "b6b7aae7-4e18-401d-aff4-64ca7996b8d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://media.delishkitchen.tv/article/742/5mvvi83j09h.jpeg?version=1625233091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  10%|█         | 1/10 [00:00<00:04,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.aomori-ringo.or.jp/kids/wp-content/uploads/2021/11/apple.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  20%|██        | 2/10 [00:00<00:03,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://images.yogajournal.jp/article/177269/5hEMJRXe8JnM3AH7K4XuE4kXw8DY5uXhHnmfKumI.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  30%|███       | 3/10 [00:01<00:03,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://upload.wikimedia.org/wikipedia/commons/1/15/Red_Apple.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  40%|████      | 4/10 [00:03<00:06,  1.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://video.kurashiru.com/production/articles/992cc573-5d02-4438-b412-35035b68c59b/wide_thumbnail_normal.jpg?1701853346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  50%|█████     | 5/10 [00:03<00:04,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.asahi-kasei.co.jp/saran/assets/images/preservation/fruits/detail/food11_tph00.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  60%|██████    | 6/10 [00:04<00:03,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://media.delishkitchen.tv/article/975/fd3sgtfnxen.jpeg?version=1634782151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  70%|███████   | 7/10 [00:05<00:01,  1.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.kewpie.co.jp/ingredients/cat_assets/img/fruits/apple/photo01.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  80%|████████  | 8/10 [00:05<00:01,  1.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.hyponex.co.jp/yasai_daijiten/websys/wp-content/uploads/2020/09/%E3%83%AA%E3%83%B3%E3%82%B4A-676x451.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for りんご:  90%|█████████ | 9/10 [00:05<00:00,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://kumamoto-illust.com/wp/wp-content/uploads/2019/07/apple-1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images for りんご: 100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n",
            "Downloading images for みかん:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://video.kurashiru.com/production/articles/35d5215c-a922-496b-8a8b-273d2215320b/wide_thumbnail_normal.jpg?1700538093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  10%|█         | 1/10 [00:00<00:02,  3.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://media.delishkitchen.tv/article/771/ax1olt9ymf.jpeg?version=1625318156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  20%|██        | 2/10 [00:00<00:02,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.ito-noen.com/dictionary/wp-content/uploads/2022/08/top-1-682x426.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  30%|███       | 3/10 [00:01<00:04,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.lettuceclub.net/i/N1/1120917/11448105.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  40%|████      | 4/10 [00:02<00:03,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://media.delishkitchen.tv/article/1745/x8mcmzheq0r.jpeg?version=1656570894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  50%|█████     | 5/10 [00:02<00:02,  1.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.cotta.jp/special/article/wp-content/uploads/2023/02/20230125_sara_1.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  60%|██████    | 6/10 [00:02<00:01,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download image 5 for みかん: HTTP status code 403\n",
            "https://img.cpcdn.com/recipes/2418134/750x750cq60/7631c8ef49af9c6af253907918082245?p=1385450195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  70%|███████   | 7/10 [00:03<00:01,  1.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.kajuen.co.jp/introduction/img/ichiran_01.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  80%|████████  | 8/10 [00:04<00:01,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://losszero.jp/cdn/shop/articles/col_089-1.jpg?v=1690788626&width=1100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDownloading images for みかん:  90%|█████████ | 9/10 [00:04<00:00,  1.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://media.delishkitchen.tv/article/1880/tbev98jjsk.jpeg?version=1666679386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images for みかん: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download the following images:\n",
            "Keyword: みかん, Image ID: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**AZURE Bing Searchを用いる方法**\n",
        "\n",
        "準備：\n",
        "- Microsoft AZUREに登録\n",
        "\n",
        "    https://learn.microsoft.com/ja-jp/azure/cognitive-services/bing-web-search/\n",
        "\n",
        "- 左のタブ → リソースの作成 → Bing Search v7を取得\n",
        "\n",
        "- ダッシュボード → キーとエンドポイントからキーを取得する"
      ],
      "metadata": {
        "id": "539gH-qvrOrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Deep_learning/api.txt\") as file:\n",
        "    #text = file.read()\n",
        "    i=1\n",
        "    key = []\n",
        "    while True:\n",
        "        include_break_line = file.readline() #改行が含まれた行\n",
        "        line = include_break_line.rstrip() #改行を取り除く\n",
        "        if line: #keyの読み込み\n",
        "            #print(f'{i}行目：{line}')\n",
        "            key.append(line)\n",
        "            i += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "bing_api_key = key[13]"
      ],
      "metadata": {
        "id": "Sa4lK_QK7-Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2e0e2e-896f-4937-bd0a-f785b54eda9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "from requests import exceptions\n",
        "import argparse\n",
        "import requests\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "API_KEY = f\"{bing_api_key}\"\n",
        "MAX_SIZE = 10\n",
        "GROUP_SIZE = 5\n",
        "\n",
        "# 取得したエンドポイントURL\n",
        "URL = \"https://api.bing.microsoft.com/v7.0/images/search\"\n",
        "OUTPUT = '/content/save_dir'\n",
        "\n",
        "if not os.path.isdir(OUTPUT):\n",
        "    os.mkdir(OUTPUT)\n",
        "\n",
        "EXCEPTIONS = set([IOError, FileNotFoundError,\n",
        "    exceptions.RequestException, exceptions.HTTPError,\n",
        "    exceptions.ConnectionError, exceptions.Timeout])\n",
        "\n",
        "search_terms = [\"forest\", \"river\", \"house\"]\n",
        "\n",
        "# set the output csv file name\n",
        "csv_file = \"url_list.csv\"\n",
        "\n",
        "# create the csv file and write the headers\n",
        "with open(csv_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Search term', 'Image URL'])\n",
        "\n",
        "# loop over each search term and download images\n",
        "for term in search_terms:\n",
        "    print(f\"[INFO] searching Bing API for '{term}'\")\n",
        "\n",
        "    # create the directory to save the images for the current search term\n",
        "    output_dir = os.path.join(OUTPUT, term)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    headers = {\"Ocp-Apim-Subscription-Key\": API_KEY}\n",
        "    params = {\"q\": term, \"offset\": 0, \"count\": GROUP_SIZE, \"imageType\": \"Photo\", \"color\": \"ColorOnly\"}\n",
        "\n",
        "    # make the search\n",
        "    search = requests.get(URL, headers=headers, params=params)\n",
        "    search.raise_for_status()\n",
        "\n",
        "    # grab the results from the search, including the total number of\n",
        "    # estimated results returned by the Bing API\n",
        "    results = search.json()\n",
        "    est_num_results = min(results[\"totalEstimatedMatches\"], MAX_RESULTS)\n",
        "    print(f\"[INFO] {est_num_results} total results for '{term}'\")\n",
        "\n",
        "    # initialize the total number of images downloaded thus far\n",
        "    total = 0\n",
        "\n",
        "    # loop over the estimated number of results in `GROUP_SIZE` groups\n",
        "    for offset in range(0, est_num_results, GROUP_SIZE):\n",
        "        # update the search parameters using the current offset, then\n",
        "        # make the request to fetch the results\n",
        "        params[\"offset\"] = offset\n",
        "        search = requests.get(URL, headers=headers, params=params)\n",
        "        search.raise_for_status()\n",
        "        results = search.json()\n",
        "\n",
        "        # loop over the results\n",
        "        for v in results[\"value\"]:\n",
        "            # try to download the image\n",
        "            try:\n",
        "                # make a request to download the image\n",
        "                print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
        "                r = requests.get(v[\"contentUrl\"], timeout=30)\n",
        "\n",
        "                # build the path to the output image\n",
        "                ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
        "                filename = f\"{term}_{str(total).zfill(3)}{ext}\"\n",
        "                output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "                # write the image to disk\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(r.content)\n",
        "\n",
        "                # write the URL to the csv file\n",
        "                with open(csv_file, 'a', newline='') as f:\n",
        "                    writer = csv.writer(f)\n",
        "                    writer.writerow([term, v[\"contentUrl\"]])\n",
        "\n",
        "            # catch any errors that would not unable us to download the\n",
        "            # image\n",
        "            except Exception as e:\n",
        "                print(f\"[INFO] skipping: {v['contentUrl']}\")\n",
        "\n",
        "            # if we have reached the maximum number of images, break out\n",
        "            # of the loop\n",
        "            total += 1\n",
        "            print(f\"{total} images downloaded!\")\n",
        "            if total >= MAX_SIZE:\n",
        "                break\n",
        "\n",
        "        # if we have reached the maximum number of images, break out of\n",
        "        # the loop\n",
        "        if total >= MAX_SIZE:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "c_NgBbvCwOrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chromedriverを用いる方法**"
      ],
      "metadata": {
        "id": "e0WP5ZQnAfIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium==4.1.0 #新しいバージョンだとエラーが出るので旧バージョンにする"
      ],
      "metadata": {
        "id": "9frhTgD4BYLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これだとサムネイルしか取得できない\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Search query\n",
        "search_query = \"flowers\"\n",
        "\n",
        "# Number of images to download\n",
        "num_images = 10\n",
        "\n",
        "# Create a new folder for the images\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "# URL to search Google Images\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using Beautiful Soup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image tags\n",
        "images = soup.find_all('img')\n",
        "\n",
        "# Iterate through the images and download them\n",
        "for i, img in enumerate(images[:num_images]):\n",
        "    url = img['src']\n",
        "    print(i)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        open(f\"{search_query}/{search_query}_{i}.jpg\", \"wb\").write(response.content)\n",
        "    except:\n",
        "        print(\"download error\")"
      ],
      "metadata": {
        "id": "YRrPYIguIEBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!curl -O https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip #Chromeのバージョンに合ったchromedriverのアドレスを設定\n",
        "!unzip chromedriver_linux64.zip\n",
        "!chmod +x chromedriver\n",
        "!mv chromedriver /usr/local/bin/\n",
        "!pip install selenium\n",
        "\n",
        "from selenium import webdriver\n",
        "\n",
        "# Chromeドライバーの設定\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--disable-browser-side-navigation')\n",
        "\n",
        "# Googleで検索する\n",
        "search_query = 'flowers'\n",
        "url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
        "browser = webdriver.Chrome('chromedriver',options=options)\n",
        "browser.get(url)\n",
        "\n",
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import base64\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# 画像のURLを取得する\n",
        "soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "img_tags = soup.find_all('img', class_='rg_i')\n",
        "\n",
        "\n",
        "urls = []\n",
        "for img in img_tags:\n",
        "    try:\n",
        "        urls.append(img[\"src\"])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# 画像をダウンロードする\n",
        "if not os.path.exists(search_query):\n",
        "    os.makedirs(search_query)\n",
        "\n",
        "num_images = 10\n",
        "\n",
        "counter = 0\n",
        "for i in range(num_images):\n",
        "    print(urls[i])\n",
        "    image_data = base64.b64decode(urls[i].split(',')[1])\n",
        "\n",
        "    # バイナリデータをBytesIOオブジェクトに書き込む\n",
        "    image_stream = BytesIO(image_data)\n",
        "\n",
        "    # PILで画像オブジェクトを作成する\n",
        "    image = Image.open(image_stream)\n",
        "    image_format = image.format\n",
        "\n",
        "    # 画像のネーミング\n",
        "    num= \"{:04d}\".format(i)\n",
        "    file_name = f\"{search_query}_{num}\"\n",
        "    new_image_path = f\"{search_query}/{file_name}.{image_format}\"\n",
        "\n",
        "\n",
        "    # Save image to file\n",
        "    image.save(new_image_path)\n"
      ],
      "metadata": {
        "id": "RupG3_zyQ7QT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}